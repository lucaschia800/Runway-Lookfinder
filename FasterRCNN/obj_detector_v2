import os
import random
import copy
from PIL import Image
import io
import h5py
import numpy as np

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.utils.data import Dataset, DataLoader, Subset
from torch.cuda.amp import autocast, GradScaler

import albumentations as A
from albumentations.pytorch import ToTensorV2
from torchmetrics.detection import MeanAveragePrecision


class CustomDataset(Dataset):
    """Custom dataset for loading images and annotations from HDF5 files."""
    
    def __init__(self, hdf5_file, transforms=None):
        self.hdf5_file = h5py.File(hdf5_file, 'r')
        self.transforms = transforms
        self.images = self.hdf5_file['images']
        self.bounding_boxes = self.hdf5_file['bounding_boxes']
        self.labels = self.hdf5_file['labels']

    def __getitem__(self, idx):
        # Load image
        image = Image.open(io.BytesIO(self.images[idx])).convert("RGB")
        image = np.array(image)

        # Load annotations
        boxes = np.array(self.bounding_boxes[idx], dtype=np.float32).reshape(-1, 4)
        labels = [int(label) for label in self.labels[idx].decode('utf-8').split(",")]

        # Apply transformations
        if self.transforms:
            transformed = self.transforms(image=image, bboxes=boxes, labels=labels)
            image = transformed['image']
            boxes = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)
            labels = torch.as_tensor(transformed['labels'], dtype=torch.int64)
        else:
            # Convert to tensor if no transforms
            image = F.to_tensor(image)
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels
        }

        return image, target

    def __len__(self):
        return len(self.images)


def get_transform(train):
    """Return appropriate transforms for training/validation."""
    if train:
        return A.Compose([
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.2),
            ToTensorV2()
        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))
    
    return A.Compose([
        ToTensorV2()
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))


def get_model(num_classes):
    """Initialize Faster R-CNN model with modified classifier head."""
    # Load pre-trained model
    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)

    # Replace classifier head
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # Fine-tuning configuration
    for name, param in model.named_parameters():
        if any(layer in name for layer in ["layer3", "layer4"]):  # Corrected layer names
            param.requires_grad = True
        else:
            param.requires_grad = False

    return model


def train_model(model, data_loader, val_loader, optimizer, device, batch_size, num_epochs=75):
    """Main training loop with early stopping and mixed precision training."""
    model.to(device)
    best_loss = float('inf')
    best_model_weights = None
    patience = 10
    scaler = GradScaler()  # Fixed GradScaler initialization

    for epoch in range(num_epochs):
        model.train()
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        
        # Training phase
        for i, (images, targets) in enumerate(data_loader):
            if i % 50 == 0:
                print(f'Processed {i * batch_size} samples')

            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            with autocast():
                loss_dict = model(images, targets)
                losses = sum(loss for loss in loss_dict.values())

            scaler.scale(losses).backward()
            scaler.step(optimizer)
            scaler.update()

        # Validation phase
        val_loss = validate(model, val_loader, device)
        compute_map(model, val_loader, device)

        # Early stopping check
        if val_loss < best_loss:
            best_loss = val_loss
            best_model_weights = copy.deepcopy(model.state_dict())
            patience = 10
        else:
            patience -= 1
            if patience == 0:
                print(f'Early stopping triggered at epoch {epoch+1}')
                return best_model_weights

    return model.state_dict()


def validate(model, data_loader, device):
    """Calculate validation loss."""
    model.eval()  # Fixed: set to evaluation mode
    val_loss = 0.0
    
    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            
            loss_dict = model(images, targets)
            val_loss += sum(loss for loss in loss_dict.values()).item()

    avg_loss = val_loss / len(data_loader)
    print(f'Validation Loss: {avg_loss:.4f}')
    return avg_loss


def compute_map(model, data_loader, device):
    """Calculate mean average precision metric."""
    metric = MeanAveragePrecision(iou_type='bbox')
    model.eval()
    
    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]
            predictions = model(images)
            metric.update(predictions, targets)
    
    results = metric.compute()
    print(f"mAP: {results['map']:.4f}, mAP_50: {results['map_50']:.4f}")
    return results


if __name__ == "__main__":
    # Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 16
    num_classes = 14

    # Dataset preparation
    train_data = CustomDataset(
        hdf5_file='h5 files/train.h5',
        transforms=get_transform(train=True)
    )
    val_data = CustomDataset(
        hdf5_file='h5 files/validation.h5',
        transforms=get_transform(train=False)
    )

    # Create subset datasets
    train_subset = Subset(train_data, random.sample(range(170000), 2000))
    val_subset = Subset(val_data, random.sample(range(4000), 200))

    # Data loaders with proper collate function
    collate_fn = lambda x: tuple(zip(*x))
    train_loader = DataLoader(
        train_subset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=6,
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_subset,
        batch_size=batch_size,
        num_workers=6,
        collate_fn=collate_fn
    )

    # Model initialization
    model = get_model(num_classes)

    # Optimizer configuration
    optimizer = torch.optim.AdamW([
        {'params': model.backbone.body.parameters(), 'lr': 7.14e-5},
        {'params': model.backbone.fpn.parameters(), 'lr': 7.14e-5},
        {'params': model.rpn.parameters(), 'lr': 7.14e-4},
        {'params': model.roi_heads.parameters(), 'lr': 7.14e-2}
    ], weight_decay=1e-2)

    # Training execution
    trained_weights = train_model(
        model,
        train_loader,
        val_loader,
        optimizer,
        device,
        batch_size,
        num_epochs=75
    )

    # Save final model
    torch.save(trained_weights, 'rcnn_resnet_model.pth')